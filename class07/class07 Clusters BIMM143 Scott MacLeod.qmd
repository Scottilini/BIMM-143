---
title: "class07 Clustering BIMM143 Scott MacLeod"
author: "Scott MacLeod"
format: pdf
---

# Clustering Methods

The broad goal here is to find groupings (clusters) in your input data.

## Kmeans

First, let's make up some data to cluster. 

We are going to use `rnorm()` in order to make up some numbers. For example:
```{r}
rnorm(5)
```

But, we are going to use 1,000 and turn it into a histogram!

```{r}
x <- rnorm(1000)
hist(x)
```
Make a vector of length 60 with 30 points centered at -3 and 30 points centered at +3
```{r}
tmp <- c(rnorm(30, mean=-3), rnorm(30, mean = 3))
tmp

```
I will now make a small x and y dataset with 2 groups of points. Basically going to take the reverse of it! We are going to use the `rev()` function. 

```{r}
x <-cbind(x=tmp, y=rev(tmp))
x
plot(x)
```
We are going to run `kmeans()`
```{r}
k <- kmeans(x, centers = 2)
k
```

Q. From your result object `k` how many points are in each cluster?
```{r}
k$size
```

Q. What "component" of your results object details the cluster membership?
```{r}
k$cluster
```

Q. Cluster centers?
```{r}
k$centers
```

Q. Plot of our clustering results?
```{r}
plot(x, col=k$cluster)
points(k$centers, col=5, pch=15, cex=2)
```
We can also cluster into 4 groups!
```{r}
#kmeans
k4 <- kmeans(x, centers = 4)
k4
#plot results
plot(x, col=k4$cluster)
```

A big limitation of `kmeans()` is that it does wht you ask even if you ask for silly clusters!


## Hierarchical Clustering

The main base R function for Hierarchical Clustering is `hclust()`. Unlike `kmeans()` you can not just pass your data as input. You first need to calculate a distance matrix.

```{r}
d <- dist(x)
hc <- hclust(d)
hc
```

Use `plot()` to view results.
```{r}
plot(hc)
abline(h=8, col=5)
```

To make the "cut" and get our cluster membership vector we can use the `cutree()` function.

```{r}
grps <- cutree(hc, h=8)
grps
```

Make a plot of our data colored by hclust results.
```{r}
plot(x, col=grps)
```


## LAB 7 Differences in Food in the UK - Principal Component Analysis (PCA)

Here we will do PCA on some food data from the United Kingdom.



```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
x
```

**Q1.** How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?
```{r}
nrow(x)
ncol(x)
```
There are 17 rows and 4 columns. You can also look at the "environment" tab in the top right of the window.

**Q2.** Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

I like using the `row.names()` function versus subtracting the rows. If you kept subtracting the columns, eventually you would run out of columns if you kept running the code. The `row.names()` approach is more robust and leaves less room for accidents in the future.

**Q3.** Changing what optional argument in the above barplot() function results in the following plot?
```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```
```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```
By changing the "Beside" from True to False, you can change the plots. If it is false, the columns of heights are portrayed as stacked bars, and if true the columns are portrayed as juxtaposed bars. 

**Q5.** Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(10), pch=16)
```
This would be useful for a small set of Data. This Paris Plot is comparing all of the 4 countries. If we wanted to look further, we could color code the dots and see which foods/bevs are being compared. 

##PCA to the rescue 

The main "base" R function for PCA is called `prcomp()`.
We are going to use `t()` to transpose the data set. Then we took the pcr and then made a summary table of the results.

```{r}
pca <- prcomp(t(x))
summary (pca)
```
>Q How much variance is captured in two PCs?

96.5% is captured in two PCs. Look at the cumunulative tab in the table above :)

**Now to** make our main "PC score plot" (a.k.a "PC1 vs. PC2", or "PC plot" or "Ordination plot"). This thing has a lot of different names.
```{r}
attributes(pca)
```
We are after the `pca$x` result component to make our main PCA plot.
```{r}
pca$x
```
```{r}
mycols <- c("orange","red","blue","darkgreen")
plot(pca$x[,1], pca$x[,2], col=mycols, pch=16, xlab="PC1 (67.4%)", ylab="PC2(29%)")
```


Another important result from PCA is how the original variables (in this case: the foods) contributed to the PCs.

This is contained in the `pca$rotation()` object - people often call this the "loadings" or "contributions" to the PCs. 

```{r}
pca$rotation
```
We can make a plot along PC1.This one isn't as pretty as it could be, but we have the general idea.

```{r}
library(ggplot2)

contrib <- as.data.frame(pca$rotation)
ggplot(contrib) +
  aes(PC1, rownames(contrib)) +
  geom_col()
```


















