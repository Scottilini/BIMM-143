---
title: "Class 08 Breast Cancer Mini Project BIMM 143"
author: "Scott MacLeod"
format: pdf
---

## Class 08 Breast Cancer Mini Project

We are going to be using data from a study from the state of Wisconsin about Breast Cancer.

First we downloaded the data from the lab and put it into the file. We still need to extract the data from the files.

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
# Let's look at just the first 6 rows!
head(wisc.df)
```
Next, we want to remove the "diagnosis" column because it will give us all of the answers. We want to use data to answer the questions instead. So let's remove it.

```{r}
#this code removes it
wisc.data <- wisc.df[,-1]
#we are also going to create a vector for it incase we need it for later
diagnosis <- wisc.df$diagnosis
colors <- rep("black", length(wisc.df$diagnosis))
colors[wisc.df$diagnosis == "M"] <- "red"
#let's look at the first 6 again with the new data set
head(wisc.data)
#look at how it changed!
```
>Q1. How many observations are in this dataset?

569 observations.

>Q2. How many of the observations have a malignant diagnosis?

```{r}
malignant_diagnosis <- diagnosis == "M"
sum(malignant_diagnosis)
```


>Q3. How many variables/features in the data are suffixed with _mean?

10

## 2. Principal Component Analysis (PCA)

We are going to check the columns and the standard deviations. It's good that we already removed the wisc.df$diagnosis because those are not numbers.

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data, 2, sd)
```

Now we are going to the PCA

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale=TRUE)
#Let's check the summary too!
summary(wisc.pr)
```
>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27%

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs (we reach 72.636% in PC3)

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs (we reach 91.010% in PC7)

We are going to create a biplot using the `biplot()` function. 

```{r}
biplot(wisc.pr)
```

>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

This is terrible. It is a jumbled mess of labels and plot points. We need to just to PC1 and PC2 in order to condense the data!

We are going to fix this by only using PC1 and PC2.

```{r}
# Scatter plot observations by components 1 and 2
plot(wisc.pr$x, col=colors, xlab = "PC1", ylab = "PC2")
```
>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,c(1,3)], col = colors, 
     xlab = "PC1", ylab = "PC3")
```

I noticed that the plots are similar but the fist plot is a lot cleaner. But both the plots are capturing a separation in the data between the benign (black) and malignant (red).



Now we are going to visulize using `ggplot()`! 

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

Next we are going to calculate the variance!

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Next we are going to calculate the variance explained by each principal component. Then we are going to plot it!

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

Here is an alternative to visualize the same data.

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

I am going to skip the optional part of exploring extra graphs.

>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

We need to find the loading vector for the feature `concave.points_mean` 

```{r}
loading_vector_pc1 <- wisc.pr$rotation[, 1]
loading_value_concave_points_mean <- loading_vector_pc1['concave.points_mean']
loading_value_concave_points_mean
```



>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

at PC5 or you need 5 PC's to explain 80% of the data



## Hierarchical Clustering

We are going to be using hierarchical clustering!

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
#now calculate the distances
data.dist <- dist(data.scaled)
#now put them into clusters
wisc.hclust <- hclust(data.dist, method="complete")
```
>Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

The height would have to be 19!

We are going to cut up the clusters so there are 4!

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, h=19)
#let's table it!
table(wisc.hclust.clusters, diagnosis)
```
>Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?


```{r}
wisc.hclust.clusters2 <- cutree(wisc.hclust, h=13)
#let's table it!
table(wisc.hclust.clusters2, diagnosis)
```
If you cut at 13 you will get 10 clusters and if you cut at 24, you will get 2 clusters. Looking more at these you could explore how different clusters could effect the data.


>Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning!

I like ward.D2 the best because it provides tends to produce compact, well-separated clusters, which is  preferable.

## Combining Methods

We are going to redo the plot. Instead of using the complete method, we are going to use ward.D2
```{r}
wisc.hclust.ward <- hclust(data.dist, method="ward.D2")
plot(wisc.hclust.ward)
```

This is a lot more promising. Next we are going to figure if those two main branches are malignant or benign!

```{r}
grps <- cutree(wisc.hclust.ward, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```
```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=colors)
```
```{r}
g <- as.factor(grps)
levels(g)
g <- relevel(g,2)
levels(g)
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```

```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(data.dist, method="ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
```
>Q15. How well does the newly created model with four clusters separate out the two diagnoses?

They are split up in the same exact way!

>Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.


I don't have the wisc.km$cluster because I skipped the optional section. But looking at the table of wisc.hclust.clusters, we can see that they are cut up and split more. 
```{r}
table(wisc.hclust.clusters, diagnosis)
```
## Sensitivity/Specificity 

>Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

The best sensitivity would be the flipped data set because you can more easily detect the unhealthy patients. The best for specificity would be the regular data set.

## Prediction

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```
```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

>Q18. Which of these new patients should we prioritize for follow up based on your results?

We should look at patient number 2 because they most like have a lump that is actually cancerous!





